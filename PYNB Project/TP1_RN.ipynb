{"cells":[{"cell_type":"markdown","metadata":{"id":"F9VoobpbIESr"},"source":["# Redes Neuronales 2021 \n","Integrantes de grupo:\n","- Crespo, Pilar\n","- Müller, Malena\n","- Scala, Tobías\n","## TP1: Sesgos en el dataset de SNLI\n","\n","Uno de los datasets más famosos de Natural Language Inference es SNLI. En esta tarea se debe responder, dadas dos frases A y B, si B es implicación de A (\"entailment\"), B es contradictorio con A (\"contradiction\") o si lo que enuncia B es neutral respecto de A (\"neutral\"). Se dice que A es la premisa y B es la hipótesis.\n","\n","En Gururangan et al., 2018 mostraron que este dataset tiene algunos sesgos, provocados por ejemplo por las heurísticas que tienen los humanos para generar estos pares de frases (A, B). Para ello, desarrollaron un modelo que aún sin observar la premisa A pudiera clasificar el par (A, B) en alguna de las tres clases del dataset.\n","\n","En este trabajo práctico intentaremos predecir a qué clase pertenece cada una de las hipótesis sin observar la premisa.\n","\n","Hemos obtenido informacion sobe las metricas empleadas en los siguientes enlaces:\n","\n","https://sitiobigdata.com/2019/01/19/machine-learning-metrica-clasificacion-parte-3/#\n","\n","https://www.iartificial.net/precision-recall-f1-accuracy-en-clasificacion/\n","\n","Cabe mencionar que en el presente programa se está utilizando el dataset descargado en el siguiente link: https://nlp.stanford.edu/projects/snli/\n","\n","No se está utilizando los archivos hdf5 que se encuentran en Kaggle porque no se encontró forma de extraerles los datos. Además en Kaggle se tuvo el problema de que no se ha podido descargar la librería Keras para poder implemantar el modelo MLP. Debido a esto, se continuó con el TP con la plataforma Visual Studio Code y con el dataset descargado en el link mencionado."]},{"cell_type":"markdown","metadata":{"id":"l3oCpda6IES6"},"source":["# Importación de datos"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1633490335968,"user":{"displayName":"Elisabet del Pilar Crespo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNDU7A0PVat05kGsk4gd4vIHokmKr7gjt9r_rOBw=s64","userId":"03796753635876630620"},"user_tz":180},"id":"ekMFNDvzIES9","outputId":"8460af37-e556-4bb3-abe9-56ffa46ccde5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to C:\\Users\\Tobias\n","[nltk_data]     Scala\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to C:\\Users\\Tobias\n","[nltk_data]     Scala\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to C:\\Users\\Tobias\n","[nltk_data]     Scala\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","from nltk import data\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import json\n","\n","\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()"]},{"cell_type":"markdown","metadata":{"id":"SYfEA-QjIETB"},"source":["Importamos el archivo original del dataset para el procesamiento de datos y nos quedamos con las labels y las segundas oraciones de cada caso, ya que son las hipótesis. La clasificación de a qué clase pertenece cada par de oraciones se hace a partir del análisis de la oración que es la hipótesis."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":284,"status":"error","timestamp":1633491968696,"user":{"displayName":"Elisabet del Pilar Crespo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNDU7A0PVat05kGsk4gd4vIHokmKr7gjt9r_rOBw=s64","userId":"03796753635876630620"},"user_tz":180},"id":"01axZJ6JIETC","outputId":"47c1d55c-1bbc-4262-814b-31c701530b2d"},"outputs":[],"source":["file = pd.read_json('snli_1.0_train.jsonl', lines=True)\n","fileClass = file['gold_label']\n","fileLines = file['sentence2']\n","#print(fileClass)"]},{"cell_type":"markdown","metadata":{"id":"yeWldOcOIETE"},"source":["# Filtrado / procesamiento de datos \n","\n","Tenemos dos tipos de data set. En uno realizamos lo siguiente:\n","- word_tokenize: Separamos la oración en strings.\n","- isalpha: Se eliminan los números.\n","- lemmatize: Se pasa todo a singular y se generaliza el género.\n","- stem: Se pasan todos los verbos a infinitivo y se deja todo en minúscula.\n","\n","Y el otro dataset lo obtenemos de la misma forma que el recién mencionado, pero agregándole también la eliminación de stopwords (palabras comunes).\n","\n","Si bien inicialmente probamos eliminando las stopwords, determinamos que convenía no hacerlo ya que hay palabras comunes como \"no\", que serían eliminadas, pero en nuestro caso son necesarias estas palabras para determinar que una hipótesis corresponde a una \"contradicción\", por ejemplo. Para esto, nos basamos en el paper \"Annotation Artifacts in Natural Language Inference Data\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6oKUeu7IETG"},"outputs":[],"source":["linesFilt = []\n","for i in range(len(fileLines)):\n","    if (i % 1000 == 0):\n","        print(i)\n","    tok=word_tokenize(fileLines[i]) #Separa la oración en strings.\n","    alpha=[x for x in tok if x.isalpha()] #Saca palabras con números.\n","    lem=[lemmatizer.lemmatize(x,pos='v') for x in alpha] #Pasa de plural a singular y generaliza el género.\n","    #stop=[x for x in lem if x not in stopwords.words('english')] #Saca las palabras comunes.\n","    stem=[stemmer.stem(x) for x in lem] #Verbos a infinitivo y pasa todo a minuscula.\n","    linesFilt.append(\" \".join(stem))"]},{"cell_type":"markdown","metadata":{"id":"hLtDLmj8IETJ"},"source":["Guardamos en un json las hipotesis ya procesadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8aq_VTmIETL"},"outputs":[],"source":["with open('train_processed_.jsonl', 'w') as file:\n","    for i in range(len(fileClass)):\n","        data2jsonl = {'gold_label': fileClass[i], 'sentence2': linesFilt[i]}\n","        json.dump(data2jsonl, file)\n","        file.write('\\n')"]},{"cell_type":"markdown","metadata":{"id":"_5wvYT5rIETP"},"source":["Repetimos lo anterior para los datasets de validación y de test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnajVnBGIETR"},"outputs":[],"source":["# ---- Validation file:\n","file = pd.read_json('snli_1.0_dev.jsonl', lines=True)\n","fileClass = file['gold_label']\n","fileLines = file['sentence2']\n","print(fileClass)\n","\n","linesFilt = []\n","for i in range(len(fileLines)):\n","    if (i % 1000 == 0):\n","        print(i)\n","    tok=word_tokenize(fileLines[i]) #Separa la oración en strings.\n","    alpha=[x for x in tok if x.isalpha()] #Saca palabras con números.\n","    lem=[lemmatizer.lemmatize(x,pos='v') for x in alpha] #Pasa de plural a singular y generaliza el género.\n","    #stop=[x for x in lem if x not in stopwords.words('english')] #Saca las palabras comunes.\n","    stem=[stemmer.stem(x) for x in lem] #Verbos a infinitivo y pasa todo a minuscula.\n","    linesFilt.append(\" \".join(stem))\n","\n","with open('val_processed_.jsonl', 'w') as file:\n","    for i in range(len(fileClass)):\n","        data2jsonl = {'gold_label': fileClass[i], 'sentence2': linesFilt[i]}\n","        json.dump(data2jsonl, file)\n","        file.write('\\n')\n","\n","# ---- Test file:\n","file = pd.read_json('snli_1.0_test.jsonl', lines=True)\n","fileClass = file['gold_label']\n","fileLines = file['sentence2']\n","print(fileClass)\n","\n","linesFilt = []\n","for i in range(len(fileLines)):\n","    if (i % 1000 == 0):\n","        print(i)\n","    tok=word_tokenize(fileLines[i]) #Separa la oración en strings.\n","    alpha=[x for x in tok if x.isalpha()] #Saca palabras con números.\n","    lem=[lemmatizer.lemmatize(x,pos='v') for x in alpha] #Pasa de plural a singular y generaliza el género.\n","    #stop=[x for x in lem if x not in stopwords.words('english')] #Saca las palabras comunes.\n","    stem=[stemmer.stem(x) for x in lem] #Verbos a infinitivo y pasa todo a minuscula.\n","    linesFilt.append(\" \".join(stem))\n","\n","with open('test_processed_.jsonl', 'w') as file:\n","    for i in range(len(fileClass)):\n","        data2jsonl = {'gold_label': fileClass[i], 'sentence2': linesFilt[i]}\n","        json.dump(data2jsonl, file)\n","        file.write('\\n')"]},{"cell_type":"markdown","metadata":{"id":"_FSY46PtIETU"},"source":["# Entrenamiento de la red con Naive Bayes\n","\n","Levantamos los datos ya procesados."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"elapsed":275,"status":"error","timestamp":1633491950221,"user":{"displayName":"Elisabet del Pilar Crespo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNDU7A0PVat05kGsk4gd4vIHokmKr7gjt9r_rOBw=s64","userId":"03796753635876630620"},"user_tz":180},"id":"aW9OabRKIETW","outputId":"fa4d7240-e739-4bcc-f3a8-d25e1f25bce9"},"outputs":[],"source":["import numpy as np\n","import pandas as pd #Implementación de clasificador bayesiano.\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n","# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","\n","trainFile = pd.read_json('train_processed_.jsonl', lines=True)\n","trainClass = trainFile['gold_label'].tolist()\n","trainLines = trainFile['sentence2'].tolist()\n","\n","valFile = pd.read_json('val_processed_.jsonl', lines=True)\n","valClass = valFile['gold_label'].tolist()\n","valLines = valFile['sentence2'].tolist()\n","\n","testFile = pd.read_json('test_processed_.jsonl', lines=True)\n","testClass = testFile['gold_label'].tolist()\n","testLines = testFile['sentence2'].tolist()"]},{"cell_type":"markdown","metadata":{"id":"yynHZAKTIETX"},"source":["Obtenemos las matrices dispersas que contienen la frecuencia (cantidad de ocurrencia) de cada palabra del vocabulario."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":313,"status":"error","timestamp":1633490193571,"user":{"displayName":"Elisabet del Pilar Crespo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNDU7A0PVat05kGsk4gd4vIHokmKr7gjt9r_rOBw=s64","userId":"03796753635876630620"},"user_tz":180},"id":"FOdatuOaIETY","outputId":"ea37fa98-c587-474f-a7f1-dcf01146c8a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["(550152, 56323)\n","(10000, 56323)\n","(10000, 56323)\n"]}],"source":["# countVect = CountVectorizer(max_df=0.8,min_df=10, ngram_range=(1,2)) #Recibe todos los artículos y arma los vectores de cuenta. Check \"ngram_range\"\n","tfidfVect = TfidfVectorizer(max_df=0.8,min_df=5, ngram_range=(1,2))#, max_features=1000) #As tf–idf is very often used for text features\n","#tfidfVect = TfidfVectorizer(max_df=0.25, min_df=1, ngram_range=(1,5))#, max_features=1000) #As tf–idf is very often used for text features\n","\n","# trainData = countVect.fit_transform(trainLines) #Learn a vocabulary dictionary of all tokens in the raw documents and return document-term matrix.\n","# print(trainData.shape) #En este sparse matrix se muestran las ocurrencias de cada palabra en cada linea.\n","trainData = tfidfVect.fit_transform(trainLines) #Learn vocabulary and idf from training set, return document-term matrix.\n","print(trainData.shape) #En este sparse matrix se muestran las ocurrencias de cada palabra en cada linea. Con tfidf las ocurrencias de las palabras no se representan con números enteros.\n","\n","# valData = countVect.transform(valLines) #Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit.\n","# print(valData.shape)\n","valData = tfidfVect.transform(valLines) #Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit.\n","print(valData.shape)\n","\n","# valData = countVect.transform(valLines) #Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit.\n","# print(valData.shape)\n","testData = tfidfVect.transform(testLines) #Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit.\n","print(testData.shape)"]},{"cell_type":"markdown","metadata":{"id":"m_MyCNJ0IETb"},"source":["Entrenamos el modelo utilizando Naive Bayes y obtenemos el score con los datos de train y el score con los nuevos datos (validation)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zmyoC5sbIETc","outputId":"2dedd82a-eba0-41b2-85cd-0d26f254e8eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6657032965435007\n","0.6412\n","0.6416\n"]}],"source":["multiNB = MultinomialNB(alpha=0.65)\n","multiNB.fit(trainData, trainClass)\n","\n","print(multiNB.score(trainData, trainClass))\n","print(multiNB.score(valData, valClass))\n","print(multiNB.score(testData, testClass))\n","\n","testPred = multiNB.predict(testData)\n","# df_test = pd.DataFrame(data=testPred, columns=[\"pred_labels\"],) #Armo el submission.csv\n","# #df_test.head()\n","# df_test.index.names = [\"pairID\"]\n","# df_test.to_csv(\"submission.csv\")"]},{"cell_type":"markdown","metadata":{"id":"tg6Fs2_nIETd"},"source":["Comprobamos la efectividad del modelo prediciendo los nuevos datos.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vPnyrdaArcAw"},"source":["## Metricas secundarias: Precision\n","\n","La metrica precision es la relacion $\\frac{tp}{tp + fp}$ , siendo tp el numero de correctos positivos y fp el numero de falsos positivos. Es la abilidad del clasificador de no clasificar como positivo aquello que es negativo. Es decir, es la cantidad de elementos identificados correctamente como positivos, respecto al total de elementos identificados como positivos. El major valor es un 1 y el peor valor es un 0.\n","Es el número de elementos identificados correctamente como positivo de un total de elementos identificados como positivos.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"yVqZEx5MIETe","outputId":"eef239ce-1d93-4682-9d96-f7861b811ef4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.63434903 0.64125178 0.66656366]\n"]}],"source":["from sklearn.metrics import precision_score\n","from keras.utils import np_utils\n","from sklearn.preprocessing import LabelEncoder\n","\n","enc = LabelEncoder()\n","enc.fit(testClass)\n","testClass = enc.transform(testClass)\n","testClass = np_utils.to_categorical(testClass-1) #Convert integers to dummy variables (i.e. one hot encoded).\n","enc.fit(testPred)\n","testPred = enc.transform(testPred)\n","testPred = np_utils.to_categorical(testPred) #Convert integers to dummy variables (i.e. one hot encoded).\n","\n","print(precision_score(testClass, testPred, average=None))\n","# print(precision_score(valClass, valPred, average='micro'))\n","# print(precision_score(valClass, valPred, average='macro'))\n","# print(precision_score(valClass, valPred, average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"Kv96mN7FIETg"},"source":["## Métrica secundaria: ROC-AUC \n","\n","Este método se basa en la curva ROC que se vincula con las características de funcionamiento del receptor y el área bajo esta curva. ROC es una curva que permite ver que tan efectiva es la clasificación del modelo. Se determina un umbral, de forma que los valores que resulten positivos por encima del umbral son verdaderos positivos, los valores negativos por encima del umbral son falsos positivos, los valores negativos debajo del umbral son verdaderos negativos y los valores positivos debajo del umbral son falsos negativos.\n","\n","La sensibilidad  es la proporción de verdaderos positivos sobre el total de resultados positivos.\n","La especificidad es la proporción de verdaderos negativos sobre el total de resultados negativos.\n","Si tomamos un valor de umbral menor, se obtienen más resultados negativos, de forma que aumenta la sensibilidad y disminuye la especificidad. Por el contrario, si tomamos un valor de umbral mayor, se obtienen más resultados negativos y entonces aumenta la especificidad y disminuye la sensibilidad.\n","La curva ROC es la sensibilidad en función de 1-especificidad. La sensibilidad da la tasa de verdaderos positivos mientras que 1-especificidad da la tasa de falso positivo. Es por esto que al aumentar la sensibilidad, 1-especificidad también aumenta. \n","AUC, por otro lado, es el área bajo la curva ROC. \n","\n","Ventajas de la métrica ROC-AUC:\n","* No varía respecto a la escala. Mide la efectividad de las predicciones, en lugar de sus valores absolutos.\n","* No varía respecto al umbral de clasificación. Mide la calidad de las predicciones del modelo, sin considerar el umbral de clasificación."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"viYeTyvdIETh","outputId":"c87a9814-61fa-4635-ffd7-d504293454d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7353541918174505\n"]}],"source":["from sklearn.metrics import roc_auc_score\n","\n","#clf = LogisticRegression(solver=\"liblinear\").fit(X, )\n","print(roc_auc_score(testClass, testPred, multi_class='macro'))"]},{"cell_type":"markdown","metadata":{"id":"A2nwvp-FuYEx"},"source":["## Metricas secundarias: F1-Score\n","\n","Combina las medidas de precision y recall. Esto es práctico porque hace más fácil el poder comparar el rendimiento combinado de la precisión y la exhaustividad entre varias soluciones.\n","\n","$F1 = 2 \\cdot \\frac{presicion \\cdot recall}{presicion + recall}$\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"sEqiPFMAuU3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6472\n","0.6470167470802781\n","0.6471769419857526\n"]}],"source":["from sklearn.metrics import f1_score\n","\n","print(f1_score(testClass, testPred, average='micro'))\n","print(f1_score(testClass, testPred, average='macro'))\n","print(f1_score(testClass, testPred, average='weighted'))\n"]},{"cell_type":"markdown","metadata":{"id":"LqMab-MiuvM4"},"source":["## Metricas secundarias: Recall\n","\n","\n","Es el número de elementos identificados correctamente como positivos del total de positivos verdaderos.\n","\n","$recall = \\frac{tp}{tp+fn}$\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"QMEDQ8s1upl8"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6472\n","0.6470955503129107\n","0.6472\n"]}],"source":["from sklearn.metrics import recall_score\n","\n","print(recall_score(testClass, testPred, average='micro'))\n","print(recall_score(testClass, testPred, average='macro'))\n","print(recall_score(testClass, testPred, average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"2R5EphoHIETi"},"source":["# Entrenamiento de la red con MLP\n","\n","Truncamos el vocabulario para no tener error por falta de memoria."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"CY8CKfHgIETj","outputId":"7849acf1-7cf3-4e13-bd2a-2ff3fe8a82e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["(550152, 1000)\n"]}],"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam, SGD\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.decomposition import TruncatedSVD\n","\n","trunSVD = TruncatedSVD(n_components=1000)\n","trainData = trunSVD.fit_transform(trainData)\n","testData = trunSVD.transform(testData)\n","print(trainData.shape)\n","\n","# enc = LabelEncoder()\n","enc.fit(trainClass)\n","trainClass = enc.transform(trainClass)\n","trainClass = np_utils.to_categorical(trainClass-1) #Convert integers to dummy variables (i.e. one hot encoded).\n","# valClass = enc.transform(valClass)\n","# valClass = np_utils.to_categorical(valClass-1) #Convert integers to dummy variables (i.e. one hot encoded)."]},{"cell_type":"markdown","metadata":{"id":"HsseZbaot2fl"},"source":["## Métrica primaria: Accuracy\n","\n","Accuracy es el porcentaje de elementos clasificados correctamente, por lo tanto, es un valor entre 0 y 1. Cuanto mayor es este valor, significa que mejor funciona la clasificación.\n","\n","* Ventaja: es la métrica más intuitiva y simple de usar.\n","\n","* Desventajas:\n","La predicción a partir de la clase más frecuente puede no ser buena si las clases a las que pertenecen los datos no están balanceadas."]},{"cell_type":"markdown","metadata":{"id":"ZDKtsclhIETk"},"source":["Generamos la estructura de la red neuronal con la siguiente función."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"9t-uZjpFIETl"},"outputs":[],"source":["def neuralNetwork():\n","    Nwords = trainData.shape[1] #cantidad de datos (palabras del vocabulario). ###\n","    model = Sequential()\n","    model.add(Dense(200, input_shape=(Nwords,), activation='relu'))\n","    model.add(Dense(trainClass.shape[1], activation='softmax')) #La salida de la función softmax puede ser utilizada para representar una distribución categórica. \n","                                                #Es empleada en varios métodos de clasificación multiclase tales como Regresión Logística Multinomial.\n","                                                #La función softmax es utilizada como capa final de los clasificadores basados en redes neuronales.\n","    model.summary()\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"2IBOT8FjIETm"},"source":["Entrenamos la red neuronal y obtenemos el accuracy del mismo."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Woy7O_W_IETm","outputId":"0315211c-6da6-4646-a676-4f15fd40651e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 200)               200200    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 603       \n","=================================================================\n","Total params: 200,803\n","Trainable params: 200,803\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","2150/2150 [==============================] - 13s 6ms/step - loss: 0.8676 - accuracy: 0.6030\n","Epoch 2/10\n","2150/2150 [==============================] - 12s 6ms/step - loss: 0.8344 - accuracy: 0.6215\n","Epoch 3/10\n","2150/2150 [==============================] - 12s 6ms/step - loss: 0.8131 - accuracy: 0.6354\n","Epoch 4/10\n","2150/2150 [==============================] - 12s 6ms/step - loss: 0.7946 - accuracy: 0.6465\n","Epoch 5/10\n","2150/2150 [==============================] - 12s 5ms/step - loss: 0.7787 - accuracy: 0.6554\n","Epoch 6/10\n","2150/2150 [==============================] - 12s 6ms/step - loss: 0.7645 - accuracy: 0.6634\n","Epoch 7/10\n","2150/2150 [==============================] - 12s 6ms/step - loss: 0.7519 - accuracy: 0.6707\n","Epoch 8/10\n","2150/2150 [==============================] - 11s 5ms/step - loss: 0.7401 - accuracy: 0.6766\n","Epoch 9/10\n","2150/2150 [==============================] - 12s 5ms/step - loss: 0.7295 - accuracy: 0.6825\n","Epoch 10/10\n","2150/2150 [==============================] - 13s 6ms/step - loss: 0.7196 - accuracy: 0.6881\n","313/313 [==============================] - 1s 2ms/step - loss: 0.8156 - accuracy: 0.6448\n","Test Accuracy: 0.644800\n"]}],"source":["#Cross validation.\n","# estClass = KerasClassifier(build_fn=neuralNetwork, epochs=50, batch_size=256, verbose=0) #Estimator.\n","# Kfold = KFold(n_splits=10, shuffle=True)\n","# scores = cross_val_score(estClass, trainData_.todense(), encClass_, cv=Kfold)\n","# print(\"Score: %.2f%% (%.2f%%)\" % (scores.mean()*100, scores.std()*100))\n","#Hold out\n","model = neuralNetwork()\n","model.fit(trainData, trainClass, epochs=10, batch_size=256, verbose=1)\n","loss, acc = model.evaluate(testData, testClass)\n","print('Test Accuracy: %f' % (acc))"]},{"cell_type":"markdown","metadata":{"id":"M9KkQwurIETn"},"source":["Comprobamos la eficiencia del modelo con los nuevos datos (validation).\n","## Métrica secundaria: Precisión."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ZP1Oh95HIETo","outputId":"d358f3a9-e861-4506-aaad-eefbebd5a3b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 0s 2ms/step\n","[0.71178274 0.70719424 0.67961499]\n"]}],"source":["testPred = model.predict(testData, verbose = 1)\n","testPred = testPred.round()\n","\n","print(precision_score(testClass, testPred, average=None))\n","# print(precision_score(valClass_, valPred, average='micro'))\n","# print(precision_score(valClass_, valPred, average='macro'))\n","# print(precision_score(valClass_, valPred, average='weighted'))"]},{"cell_type":"markdown","metadata":{"id":"7-3_Zc1MIETp"},"source":["## Métrica secundaria: ROC-AUC"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"1PCxHdAqIETp","outputId":"e3f673bd-680f-4d9c-f1da-1b9361bbd7ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7178211062117722\n"]}],"source":["from sklearn.metrics import roc_auc_score\n","\n","#clf = LogisticRegression(solver=\"liblinear\").fit(X, )\n","print(roc_auc_score(testClass, testPred, multi_class='macro'))"]},{"cell_type":"markdown","metadata":{"id":"ir81qUUCIETq"},"source":["# Comparacion entre las metricas empleadas\n","\n","Accuracy es la métrica más intuitiva y simple de usar, pero tiene como desventaja que la predicción a partir de la clase más frecuente puede no ser buena si las clases a las que pertenecen los datos no están balanceadas. Puede hacer creer que el modelo funciona mejor de como realmente funciona.\n","\n","Las otras metricas empleadas en este trabajo practico son mas representativas y tienen en cuenta el balance entre clases. \n","\n","Presicion da la calidad de la prediccion ya que indica, de lo que fue clasificado como que pertenece a una dada clase, el porcentaje que realmente pertenece a esa clase.\n","\n","Recall indica, del total de datos pertenecientes a una dada clase, que porcentaje pudo ser correctamente clasificado.\n","\n","F1 es una metrica que combina las metricas presicion y recall.\n","\n","Una ventaja de la métrica ROC-AUC es que no varía respecto a la escala. Mide la efectividad de las predicciones, en lugar de sus valores absolutos. Y ademas, no varía respecto al umbral de clasificación. Mide la calidad de las predicciones del modelo, sin considerar el umbral de clasificación.\n","\n","Por lo tanto, es importante tener en cuenta que es lo que se quiere rescatar del resultado de la clasificacion del modelo para determinar la metrica a usar para evaular dicho modelo. No conviene usar accuracy, a no ser que la cantidad de datos que pertenecen a cada clase este balanceada.\n","\n","\n","\n","\n","\n","\n","..."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"TP1_RN.ipynb","provenance":[]},"interpreter":{"hash":"352c8700b9e48b360dd8f8c948f408188979ddb3d67f3cf144936d3d11d35124"},"kernelspec":{"display_name":"Python 3.8.10 64-bit ('rn': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
